- name: "Yann Dubois"
  url: "https://yanndubs.github.io/"
  position: "Ph.D. Candidate"
  affiliation: "Stanford University"
  image: "images/yann_dubois.jpg"
  title: "Scalable Evaluation of Large Language Models"
  date: "Monday September 23 (virtual)"
  abstract: "Evaluation is a cornerstone of machine learning, critical for model development and selection. For LLMs like ChatGPT, evaluation presents unique challenges due to the open-ended nature of their outputs. While human evaluation remains the gold standard, its cost and time-intensive nature make it impractical for rapid development cycles. This talk will provide an overview of scalable approaches to LLM evaluation, focusing on using one LLM to evaluate another. I will discuss the potential benefits and limitations of this approach and explore mitigation strategies for its challenges."
  bio: "Yann is a final-year CS PhD student advised by Percy Liang and Tatsu Hashimoto. His research focuses on improving the effectiveness of AI when resources are scarce. Most recently, he has been part of the Alpaca team, working on training and evaluating language models more efficiently using other LLMs."
  notes: "<b>Supplementary Reading:</b> [<a href=\"https://tatsu-lab.github.io/alpaca_eval/\">AlpacaEval</a>] [<a href=\"https://arxiv.org/abs/2404.04475\">paper 1</a>] [<a href=\"https://arxiv.org/abs/2306.05685\">paper 2</a>]"
  video: "https://youtu.be/ZaQYM-YF1rM"
  slides: "slides/Yann_Dubois.pdf"

- name: "Hanjun Dai"
  url: "https://hanjun-dai.github.io/"
  position: "Staff Research Scientist & Research Manager"
  affiliation: "Google Brain"
  image: "images/hanjun_dai.jpg"
  title: "Preference Optimization for Large Language Models"
  date: "Wednesday September 25 (virtual)"
  abstract: "Reinforcement Learning (RL) plays a key role in the success of large language models (LLMs), especially the recent breakthrough in scaling the inference time computation for reasoning. In this lecture, we aim at providing a high-level overview of the RL for LLM training, including the policy and the reward modeling, the optimization and the known difficulties in applying RL, in terms of the challenges in both quality and infrastructure. We then cover the alternative approaches for aligning preferences, including direct preference optimization (DPO), best of n (BoN) and the recent advances of research in this direction. We conclude with the comparisons among these preference optimization methods, as well as some open problems in this field."
  bio: "Hanjun is a staff research scientist and research manager at Google DeepMind. He obtained his PhD from Georgia Institute of Technology. His research focuses on efficient generative modeling for text, image and structured data, and the corresponding fundamental algorithms in sampling and optimization. His research outcome has been adopted in OSS projects and launched in Google Workspace, Gemini and Cloud AI. His work has been recognized by the 2022 Google Research tech impact award, AISTATS 2016 best student paper and best workshop papers in Recsys and Neurips. He has also served as the Area Chair in top-tier conferences including AAAI, ICML, NeurIPS, co-organized workshops and tutorials in ICML, NeurIPS, LoG."
  notes: "<b>Supplementary Reading:</b> Basic topics [<a href=\"https://arxiv.org/abs/2203.02155\">paper 1</a>] [<a href=\"https://arxiv.org/abs/2305.18290\">paper 2</a>] [<a href=\"https://arxiv.org/abs/2404.10719v1\">paper 3</a>] and advanced topics [<a href=\"https://arxiv.org/abs/2310.12036\">paper 1</a>] [<a href=\"https://arxiv.org/abs/2405.19320\">paper 2</a>] [<a href=\"https://arxiv.org/abs/2407.14622\">paper 3</a>] [<a href=\"https://arxiv.org/abs/2406.00832\">paper 4</a>]"
  slides: "slides/Hanjun_Dai.pdf"

- name: "Vijay Krishnan"
  url: "http://i.stanford.edu/~kvijay/"
  position: "Founder and CTO"
  affiliation: "Turing"
  image: "images/vijay_krishnan.jpeg"
  title: "Improving Foundation Models Using Expert Human Data"
  date: "Tuesday October 8 (hybrid - Levine 101)"
  abstract: "Foundation models including LLMs and multi-modal models released by OpenAI (GPT), Anthropic (Claude), Google (Gemini), Meta (Llama), and others have shown very impressive capabilities across a range of tasks. Some key drivers of this performance — such as investments in GPUs/compute, model size, and pre-training data — are relatively well understood. This presentation will focus on a less understood, yet extremely powerful lever that creates significant differentiation and competitive advantage among state-of-the-art models: the use of expert human data for Evaluations (“Evals”), Supervised Fine Tuning (SFT), Reinforcement Learning with Human Feedback (RLHF), and Direct Preference Optimization (DPO).The talk will also outline some best practices for maximizing returns on financial investments in human data to achieve optimal model performance. This includes effective strategies for sourcing, vetting, hiring, and managing expert human data teams, as well as task design for Evals, SFT, RLHF, and DPO, along with processes and tooling to optimize team performance, data quality and throughput."
  bio: "Vijay Krishnan is Founder and CTO of Turing.com and leads its AI and machine learning initiatives. With over 20 years of experience across academia and industry, he has spearheaded large-scale machine learning projects addressing a wide range of industry challenges. Before founding Turing, Krishnan served as CTO of Rover (acquired by Revcontent), was a scientist at Yahoo, and has authored widely cited papers on ML and NLP. He holds degrees in Computer Science from Stanford University and IIT Bombay."
  notes: "<b>About Turing:</b> <a href=\"https://www.turing.com/\">Turing.com</a> enables foundation model companies like OpenAI, Anthropic, Google, Nvidia, Meta, Microsoft, and many others to improve their capabilities in areas such as reasoning, coding, function calling, multimodality, factuality, safety, and more through expert human data for Evals, SFT, RLHF, and DPO. Turing also helps large enterprises like Pepsi, Disney, and J&J strategize and execute various AI solutions to create outsized business value. Turing achieves both these through its talent pool of over 3 million software developers, data scientists, and other knowledge workers spanning 100+ countries, all automatically vetted through 100+ tests of skills and expertise. Founded in 2018, Turing became a unicorn in 2021."
  slides: "slides/Vijay_Krishnan.pdf"
  video: "https://youtu.be/P1GGfvIZ9rE"

- name: "Denny Zhou"
  url: "https://dennyzhou.github.io/"
  position: "Principal Scientist & Research Director"
  affiliation: "Google DeepMind"
  image: "images/denny_zhou.jpeg"
  title: "LLM Reasoning: Key Ideas and Limitations"
  date: "Wednesday October 9 (hybrid - Walnut 401B)"
  abstract: "Reasoning has emerged as a central focus in the development of large language models (LLMs). This talk will largely be based on the work from the reasoning team that I founded and lead in Google DeepMind. I will talk about how we started and explored reasoning with LLMs, including the key ideas like chain of thought and self consistency, and the limitations that we have observed."
  bio: "Denny Zhou is a Principal Scientist / Research Director at Google DeepMind, where he founded and leads the Reasoning Team. He received the Google Research Tech Impact Award for his work on large language models (LLMs) in 2022, and the WSDM Test of Time Award 2022. He delivered distinguished keynotes at KDD 2023's LLM Day, and the Grand Opening of Yale's Institute for Foundations of Data Science in 2023. He is serving as a General Chair for the Conference on Language Modeling (COLM) 2024."
  notes: "<b>Supplementary Reading:</b> [<a href=\"https://arxiv.org/abs/2402.10200\">paper 1</a>] [<a href=\"https://arxiv.org/abs/2402.12875\">paper 2</a>]"

- name: "Hyung Won Chung"
  url: "https://hwchung27.github.io/"
  position: "Research Scientist"
  affiliation: "OpenAI"
  image: "images/hyung_won_chung.jpeg"
  title: "Shaping the Future of AI from the History of Transformer"
  date: "Monday October 14 (virtual)"
  abstract: "AI is developing at such an overwhelming pace that it is hard to keep up. Instead of spending all our energy catching up with the latest development, I argue that we should study the change itself. First step is to identify and understand the driving force behind the change. For AI, it is the exponentially cheaper compute and associated scaling. I will provide a highly-opinionated view on the early history of Transformer architectures, focusing on what motivated each development and how each became less relevant with more compute. This analysis will help us connect the past and present in a unified perspective, which in turn makes it more manageable to project where the field is heading."
  bio: "Hyung Won Chung is a research scientist at OpenAI. His recent work focuses on o1. He has worked on various aspects of Large Language Models: pre-training, instruction fine-tuning, reinforcement learning with human feedback, reasoning, multilinguality, parallelism strategies, etc. Some of the notable work includes scaling Flan paper (Flan-T5, Flan-PaLM) and T5X, the training framework used to train the PaLM language model. Before OpenAI, he was at Google Brain and before that he received a PhD from MIT."
  notes: "<b>Supplementary Reading:</b> [<a href=\"https://arxiv.org/abs/1910.10683\">paper 1</a>] [<a href=\"https://arxiv.org/abs/1911.02150\">paper 2</a>]"

- name: "Guangxuan Xiao"
  url: "https://guangxuanx.com/"
  position: "Ph.D. Candidate"
  affiliation: "MIT"
  image: "images/guangxuan_xiao.jpg"
  title: "Efficient Large Language Model Deployment with Quantization"
  date: "Wednesday October 16 (hybrid - Walnut 401B)"
  abstract: "Large language models (LLMs) achieve state-of-the-art performance across a wide range of AI applications but demand significant compute and memory resources, limiting their deployment on edge devices and cloud servers alike. Quantization, which reduces model precision to accelerate inference and lower memory usage, offers a promising solution, yet it comes with unique challenges when applied to LLMs.<br><br> In this talk, I will present a series of cutting-edge quantization techniques developed by our lab to address these challenges. We begin with SmoothQuant, a post-training quantization method that enables INT8 weight and activation quantization, preserving accuracy and achieving up to 1.56x speedup and 2x memory reduction across models such as Llama-2, OPT, and Falcon. Then, we explore AWQ, a hardware-efficient, activation-aware weight quantization technique for low-bit, on-device LLMs, which delivers superior accuracy and more than 3x speedup on edge GPUs. Finally, we introduce QServe, an innovative W4A8KV4 quantization and system co-design solution for large-batch LLM serving, which significantly reduces GPU dequantization overhead and improves throughput by up to 3.5x compared to existing frameworks.<br><br> These advancements collectively offer practical, scalable solutions for efficient LLM deployment in both cloud and edge environments, democratizing access to powerful language models while optimizing hardware costs."
  bio: "Guangxuan Xiao is a third-year Ph.D. candidate at MIT EECS, advised by Prof. Song Han. He focuses on creating efficient algorithms for deep learning, especially for large language models (LLMs). His work has earned widespread attention, receiving over 9,000 GitHub stars and making a tangible impact on industry practices. His key contributions, including SmoothQuant and StreamingLLM, have been widely adopted and integrated into platforms such as NVIDIA's TensorRT-LLM, HuggingFace, and Intel's Neural Compressor."
  notes: "<b>Supplementary Reading:</b> [<a href=\"https://arxiv.org/abs/2211.10438\">paper 1</a>] [<a href=\"https://arxiv.org/abs/2306.00978\">paper 2</a>] [<a href=\"https://arxiv.org/abs/2405.04532\">paper 3</a>]"
  slides: "slides/Guangxuan_Xiao.pdf"

- name: "Kai Sheng Tai"
  url: "https://kaishengtai.github.io/"
  position: "Research Scientist"
  affiliation: "Meta"
  image: "images/kai_sheng_tai.jpg"
  title: "Sparsity for Efficient LLM Inference"
  date: "Monday October 21 (virtual)"
  abstract: "This lecture surveys the many faces of sparsity in the context of efficient LLM inference. First, we cover post-training pruning algorithms that zero-out 50% or more of a trained LLM's parameters while minimizing quality loss. Next, we give an overview of methods that set the sparsity pattern dynamically based on the model's input. Throughout, we will discuss the various tradeoffs that arise when deciding which of these tools to use in practice."
  bio: "Kai Sheng is a Research Scientist at Meta working on inference efficiency for on-device LLMs. Prior to Meta, he received his Ph.D. in CS from Stanford, focusing on algorithms and architectures for resource efficient machine learning."
  notes: "<b>Supplementary Reading:</b> [<a href=\"https://arxiv.org/abs/2301.00774\">paper 1</a>] [<a href=\"https://arxiv.org/abs/2209.01667\">paper 2</a>]"

- name: "Ram Sriharsha"
  url: "https://www.linkedin.com/in/harsha340/"
  position: "CTO"
  affiliation: "Pinecone"
  image: "images/ram_sriharsha.jpg"
  date: "Monday October 28 (virtual)"

- name: "Thang Luong"
  url: "http://research.google/people/105176/"
  position: "Senior Staff Research Scientist"
  affiliation: "Google DeepMind"
  image: "images/thang_luong.png"
  title: "Towards AI Superhuman Reasoning for Math and beyond"
  date: "Monday November 4 (virtual)"
  abstract: "In this talk, I will discuss the recent advances in AI for Math such as AlphaGeometry and AlphaProof. Through the talk, I will also share my perspective on the future of AI and hint towards a bigger picture of advancing reasoning capabilities of existing AI systems."
  bio: "Thang Luong is currently a Senior Staff Research Scientist and Senior Manager at Google DeepMind, formerly Google Brain. He obtained his PhD in Computer Science from Stanford University in 2016, during which he pioneered the field of deep learning for machine translation. At Google, Dr. Luong built state-of-the-art models in both language and vision such as ELECTRA and NoisyStudent. He co-founded Project Meena, which debuted the world’s best chatbot in 2020 and later became Google LaMDA chatbot in 2021. Dr. Luong has been co-leading the development of Bard Multimodality since 2022 and is the principal investigator of the AlphaGeometry project (published in Nature, 2024) that solves Olympiad-level geometry problems and competes at the International Mathematical Olympiad (IMO). Recently, he led the Superhuman Reasoning team at Google to build the first AI that reached Silver medalist level at IMO 2024."
  notes: "<b>Supplementary Reading:</b> [<a href=\"https://www.nature.com/articles/s41586-023-06747-5\">paper</a>] [<a href=\"https://deepmind.google/discover/blog/ai-solves-imo-problems-at-silver-medal-level\">blog</a>"

- name: "Jason Wei"
  url: "https://www.jasonwei.net/"
  position: "Member of the Technical Staff"
  affiliation: "OpenAI"
  image: "images/jason_wei.png"
  date: "Wednesday November 20 (hybrid - Singh Forum)"

- name: "Aakanksha Chowdhery"
  url: "https://www.achowdhery.com/"
  position: "Senior Staff Research Scientist"
  affiliation: "Meta"
  image: "images/aakanksha_chowdhery.jpeg"
  date: "Monday November 25 (virtual)"


