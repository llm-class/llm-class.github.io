---
layout: default
---

  <h2>CIS 7000: Large Language Models (Spring â€™24)</h2>
  <h3>General Information</h3>
  <ul>
    <li><strong>Instructor:</strong> Mayur Naik</li>
    <li><strong>Credit Units:</strong> 1 CU </li>
    <li><strong>Class Schedule:</strong> Lectures for 3 hours/week on Mondays and Wednesdays; occasional recitations on Fridays</li>
    <li><strong>Prerequisites:</strong>
      <ul>
        <li>CIS 5200 or equivalent: Mathematical foundations of machine learning</li>
        <li>CIS 5450 or equivalent: Experience with building, training, and debugging machine learning models</li>
      </ul>
    </li>
    <li><strong>Text/Required Materials:</strong> Selected papers from machine learning conferences
      (e.g., NeurIPS, ICML, ICLR)</li>
  </ul>

  <h3>Course Description</h3>
  <p>
    This course offers an in-depth exploration of large language models (LLMs) focusing on their design, training, and
    usage. It begins with the attention mechanism and transformer architectures, moves through practical aspects of
    pre-training and efficient deployment, and concludes with advanced techniques like prompting and neurosymbolic
    learning.
    The course aims to equip students with the skills to critically analyze LLM research and apply these concepts in
    real-world scenarios. A solid foundation in machine learning, programming, and deep learning is recommended.
  </p>
  <h3>Topics Covered</h3>
  <ul>
    <li>History of LLMs</li>
    <li>Transformer Architectures</li>
    <li>Model Training Techniques</li>
    <li>Prompt Engineering</li>
    <li>Ethical Considerations and Safety Measures</li>
    <li>Advanced Integration Techniques (e.g., RAG, Agents, Neurosymbolic Learning)</li>
  </ul>

  <h3>Weekly Schedule</h3>
  <ul>
    <li><strong>Week 1:</strong> Course Introduction</li>
    <li><strong>Weeks 2-3:</strong> The Transformer Architecture</li>
    <li><strong>Weeks 4-5:</strong> Pre-training (Data Preparation, Parallelism, Scaling Laws, Instruction Tuning, Alignment, Evaluation)</li>
    <li><strong>Week 6:</strong> Adaptation (Parameter-Efficient Fine-Tuning Techniques and Design Spaces)</li>
    <li><strong>Week 7:</strong> Prompting Techniques</li>
    <li><strong>Week 8:</strong> Fast and Efficient Inference (Quantization, vLLM Framework, Flash Attention, Sparsification/Distillation)</li>
    <li><strong>Week 9:</strong> RAG and Vector DBs</li>
    <li><strong>Week 10:</strong> Agent Frameworks</li>
    <li><strong>Week 11:</strong> Neurosymbolic Architectures</li>
    <li><strong>Weeks 12:</strong> Course Wrap-Up</li>
    <li><strong>Weeks 13-14:</strong> Project - Conception, Design, Implementation, Evaluation</li>
    <li><strong>Weeks 15-16:</strong> Project - Report and Presentation</li>
  </ul>

  <h2>Course Objectives</h2>
  <p>By the end of this course, students will be able to:</p>
  <ul>
    <li>Analyze design decisions in modern and upcoming transformer architectures.</li>
    <li>Determine the hardware, software, and data requirements for pre-training or fine-tuning an LLM for new tasks.
    </li>
    <li>Understand where LLMs should and should not be used based on their capability and reliability.</li>
    <li>Leverage a deep understanding of LLM theory and software to design prompts and applications around them.</li>
  </ul>

    <h2>Grading Details</h2>
    <p>
      The course comprises three major activities:
    </p>
    <ul>
      <li><strong>Homeworks:</strong> Programming assignments involving implementation of different LLM techniques.</li>
      <li><strong>Lectures:</strong> Covering LLM concepts supplemented with readings and guest lectures. Attendance is mandatory!</li>
      <li><strong>Project:</strong> Deep-dive into implementing and analyzing an LLM technique in teams of 2-3 students.</li>
    </ul>
    <p><strong>Grading Rubric:</strong></p>
    <ul>
      <li>55% Homeworks</li>
      <li>25% Project</li>
      <li>15% Final Exam</li>
      <li>5% Class Participation</li>
    </ul>
