---
layout: default
---

<div class='row'>
  <div class='col-md-12'>

  <h2>CIS 7000: Large Language Models (Spring 2024)</h2>

  <h5 class="header-colored">General Information</h5>
  <ul>
    <li><strong>Instructor:</strong> Mayur Naik</li>
    <li><strong>Credit Units:</strong> 1 CU </li>
    <li><strong>Class Schedule:</strong> Lectures during 1:45-3:15 pm on Mon/Wed in <a href="https://facilities.upenn.edu/maps/locations/walnut-street-3401">WLNT</a> 401B; occasional recitations on Fridays</li>
    <li><strong>Prerequisites:</strong>
      <ul>
        <li>CIS 5200 or equivalent: Mathematical foundations of machine learning</li>
        <li>CIS 5450 or equivalent: Experience with building, training, and debugging machine learning models</li>
      </ul>
    </li>
    <li><strong>Text/Required Materials:</strong> Selected papers from machine learning conferences
      (e.g., NeurIPS, ICML, ICLR)</li>
  </ul>

  <h5 class="header-colored">Course Description</h5>
  <p>
    This course offers an in-depth exploration of large language models (LLMs) focusing on their design, training, and
    usage. It begins with the attention mechanism and transformer architectures, moves through practical aspects of
    pre-training and efficient deployment, and concludes with advanced techniques like prompting and neurosymbolic
    learning.
    The course aims to equip students with the skills to critically analyze LLM research and apply these concepts in
    real-world scenarios. A solid foundation in machine learning, programming, and deep learning is recommended.
  </p>

  <h5 class="header-colored">Topics Covered</h5>
  <ul>
    <li>History of LLMs</li>
    <li>Transformer Architectures</li>
    <li>Model Training Techniques</li>
    <li>Prompt Engineering</li>
    <li>Ethical Considerations and Safety Measures</li>
    <li>Advanced Integration Techniques (e.g., RAG, Agents, Neurosymbolic Learning)</li>
  </ul>

  <h5 class="header-colored">Weekly Schedule</h5>
  <ul>
    <li><strong>Week 1:</strong> Course Introduction</li>
    <li><strong>Weeks 2-3:</strong> The Transformer Architecture</li>
    <li><strong>Weeks 4-5:</strong> Pre-training (Data Preparation, Parallelism, Scaling Laws, Instruction Tuning, Alignment, Evaluation)</li>
    <li><strong>Week 6:</strong> Adaptation (Parameter-Efficient Fine-Tuning Techniques and Design Spaces)</li>
    <li><strong>Week 7:</strong> Prompting Techniques</li>
    <li><strong>Week 8:</strong> Fast and Efficient Inference (Quantization, vLLM Framework, Flash Attention, Sparsification/Distillation)</li>
    <li><strong>Week 9:</strong> RAG and Vector DBs</li>
    <li><strong>Week 10:</strong> Agent Frameworks</li>
    <li><strong>Week 11:</strong> Neurosymbolic Architectures</li>
    <li><strong>Weeks 12:</strong> Course Wrap-Up</li>
    <li><strong>Weeks 13-14:</strong> Project - Conception, Design, Implementation, Evaluation</li>
    <li><strong>Weeks 15-16:</strong> Project - Report and Presentation</li>
  </ul>

  <h5 class="header-colored">Course Objectives</h5>
  <p>By the end of this course, students will be able to:</p>
  <ul>
    <li>Analyze design decisions in modern and upcoming transformer architectures.</li>
    <li>Determine the hardware, software, and data requirements for pre-training or fine-tuning an LLM for new tasks.
    </li>
    <li>Understand where LLMs should and should not be used based on their capability and reliability.</li>
    <li>Leverage a deep understanding of LLM theory and software to design prompts and applications around them.</li>
  </ul>

  <h5 class="header-colored">Grading Details</h5>
  <p>
    The course comprises three major activities:
  </p>
  <ul>
    <li><strong>Homeworks:</strong> Programming assignments involving implementation of different LLM techniques.</li>
    <li><strong>Lectures:</strong> Covering LLM concepts supplemented with readings and guest lectures. Attendance is mandatory!</li>
    <li><strong>Project:</strong> Deep-dive into implementing and analyzing an LLM technique in teams of 2-3 students.</li>
  </ul>
  <p><strong>Grading Rubric:</strong></p>
  <ul>
    <li>55% Homeworks</li>
    <li>25% Project</li>
    <li>15% Final Exam</li>
    <li>5% Class Participation</li>
  </ul>

  <h5 class="header-colored">Homework Collaboration Policy</h5>
  <p>Working with your peers to complete the homeworks is encouraged. Here is how you can do so in an acceptable manner:</p>
  <ul>
    <li>You should work on your own solutions and submit individually.</li>
    <li>You should never directly copy solutions from other students or resources.</li>
    <li>Do not share your code with others, and do not look at others' code.  There is one exception: if you have already finished a section of the homework, you can look at another student's code to verbally help them debug.</li>
    <li>Cite any resources that you used to develop your solutions, including articles, academic papers, and/or textbooks.</li>
    <li>List the names of the classmates that you collaborated with to complete the homework.</li>
    <li>Viewing or uploading solutions to/from sites likes Chegg and CourseHero is strictly prohibited.</li>
  </ul>
  <p>If you have any questions about this policy or seek clarification about the acceptability of a particular resource or collaboration, we encourage you to reach out to course staff on Ed.</p>

  </div>
</div>

